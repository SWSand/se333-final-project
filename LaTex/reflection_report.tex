% \documentclass{article}
% \usepackage{graphicx} % Required for inserting images

% \title{se333-final-project}
% \author{Jose }
% \date{November 2025}

% \begin{document}

% \maketitle

% \section{Introduction}

% \end{document}


\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reflection on Automated Testing Agent Development Using AI-Assisted Software Engineering}

\author{\IEEEauthorblockN{Jose Sandoval}
\IEEEauthorblockA{\textit{College of Computing and Digital Media} \\
\textit{DePaul University}\\
Chicago, IL, USA \\
jsando47@depaul.edu}
}

\maketitle

\begin{abstract}
This reflection report examines the development of an automated testing agent system using AI-assisted software engineering techniques. The project implemented a comprehensive Model Context Protocol (MCP) based testing framework that achieved 94.13\% instruction coverage on a Java codebase. Through iterative development across five phases, the system evolved from basic test generation tools to an integrated workflow incorporating Git automation, intelligent bug detection, and creative extensions for specification-based testing and code review. This report analyzes coverage improvement patterns, reflects on lessons learned about AI-assisted development, and provides recommendations for future enhancements.
\end{abstract}

\begin{IEEEkeywords}
automated testing, AI-assisted development, test coverage, Model Context Protocol, software agents
\end{IEEEkeywords}

\section{Introduction}

The proliferation of AI-assisted development tools has transformed software engineering practices, particularly in areas requiring repetitive tasks such as test generation. This project explored the application of GitHub Copilot in Agent Mode combined with Model Context Protocol (MCP) tools to create an automated testing framework for Java projects. The system was designed to iteratively improve test coverage through intelligent analysis of code structure and coverage reports.

The project targeted the Apache Commons Lang3 library, consisting of 108 Java classes, and implemented 19 MCP tools across five development phases. The initial baseline coverage of 94.00\% was improved to 94.13\% instruction coverage (51,743 out of 54,972 instructions covered), demonstrating the effectiveness of AI-assisted test generation workflows.

\section{Methodology}

The development process followed a phased approach, each phase building upon previous implementations:

\textbf{Phase 1-2: Core Testing Infrastructure} - Implemented 10 MCP tools for source code analysis, coverage reporting, and test file management. Tools included class structure analysis, coverage path detection, and uncovered method identification.

\textbf{Phase 3: Git Integration} - Developed 5 Git automation tools enabling intelligent file staging, automated commits with coverage statistics, and standardized pull request generation. These tools integrated seamlessly with the testing workflow.

\textbf{Phase 4: Intelligent Test Iteration} - Created 2 tools for bug detection from test failures and quality metrics dashboard generation. These tools enabled automated identification of source code bugs from test failures and comprehensive tracking of test quality indicators.

\textbf{Phase 5: Creative Extensions} - Implemented 2 innovative tools: a specification-based test generator using boundary value analysis and equivalence class partitioning, and an AI code review agent performing static analysis for code smells and security vulnerabilities.

The development utilized JaCoCo for coverage analysis, JUnit 4 for testing, and Maven for build automation. All tools were implemented in Python using the FastMCP framework and integrated with GitHub Copilot Agent Mode for automated execution.

\section{Results}

\subsection{Coverage Improvement Patterns}

The project demonstrated consistent coverage improvement through systematic test generation. Initial analysis identified TypeUtils as the class with lowest coverage (35.3\%), followed by ToStringBuilder (66.9\%) and Conversion (93.8\%). The iterative approach focused on classes with the highest number of uncovered instructions.

Coverage metrics showed steady improvement:
\begin{itemize}
\item Line coverage: 93.60\%
\item Branch coverage: 90.29\%
\item Instruction coverage: 94.13\% (51,743/54,972)
\end{itemize}

The most effective pattern emerged from batch processing: generating tests for 5 classes, running comprehensive test suites, analyzing coverage improvements, and iterating. This approach balanced thoroughness with efficiency, avoiding premature optimization of individual classes.

The Git integration tools proved particularly valuable, automatically committing coverage improvements with standardized messages including coverage statistics. This created a clear audit trail of progress and enabled tracking of incremental improvements.

\subsection{Lessons Learned About AI-Assisted Development}

Several key insights emerged from working with AI-assisted development tools:

\textbf{Iterative Refinement is Essential} - Initial AI-generated tests often required refinement. The most successful approach involved using AI to generate comprehensive test templates, then manually reviewing and enhancing edge cases and exception handling. This hybrid approach leveraged AI efficiency while maintaining human oversight for quality.

\textbf{Context is Critical} - The MCP tools provided structured context that significantly improved AI code generation quality. By analyzing class structures, method signatures, and existing test patterns before generation, the AI produced more relevant and comprehensive tests than when working without context.

\textbf{Automation Enables Scale} - The automated workflow enabled systematic coverage of 108 classes that would have been impractical manually. The ability to batch process multiple classes and automatically track progress transformed test generation from a manual, error-prone process to a systematic, repeatable workflow.

\textbf{Integration Complexity} - Integrating multiple tools (MCP server, GitHub Copilot, Maven, JaCoCo) required careful orchestration. Error handling and timeout management proved crucial for robust operation. The standardized return formats across MCP tools simplified integration but required upfront design consideration.

\textbf{Documentation as Code} - Maintaining comprehensive documentation alongside implementation proved essential. The agent prompts served as executable documentation, ensuring consistent tool usage and enabling knowledge transfer.

\subsection{Future Enhancement Recommendations}

Based on the project experience, several areas present opportunities for enhancement:

\textbf{Historical Coverage Tracking} - Implementing persistent storage of coverage trends over time would enable analysis of improvement patterns and identification of regression. This would support data-driven decisions about test prioritization.

\textbf{Enhanced Decision Tables} - The specification-based test generator could benefit from more sophisticated logic analysis. Machine learning approaches could identify complex conditional logic patterns and generate more comprehensive decision tables.

\textbf{Integration with External Tools} - Integrating with established code quality tools (SpotBugs, PMD, Checkstyle) would provide comprehensive quality metrics beyond coverage. This would create a unified quality dashboard.

\textbf{Automated Refactoring} - Extending the AI code review tool to suggest and potentially implement automated refactorings for detected code smells would further reduce manual effort.

\textbf{Multi-Language Support} - Extending the framework to support additional programming languages would increase applicability. The MCP protocol design facilitates such extensions.

\textbf{Performance Optimization} - Large codebases revealed opportunities for optimization. Caching analysis results and parallelizing test generation could significantly improve performance for projects with hundreds of classes.

\section{Conclusion}

This project successfully demonstrated the effectiveness of AI-assisted development for automated test generation. The implementation of 19 MCP tools across five phases created a comprehensive testing framework achieving 94.13\% instruction coverage. The iterative development approach, combined with systematic coverage analysis and intelligent automation, proved highly effective.

Key achievements include the seamless integration of testing, version control, and quality analysis tools into a unified workflow. The creative extensions demonstrated the potential for AI-assisted tools to address broader software development challenges beyond test generation.

The lessons learned highlight both the promise and limitations of current AI-assisted development tools. While AI significantly accelerates development and enables systematic approaches to large-scale tasks, human oversight and iterative refinement remain essential for quality outcomes.

Future work should focus on enhancing the intelligence of automated tools, expanding integration capabilities, and developing more sophisticated analysis techniques. The foundation established in this project provides a solid base for such enhancements.

\begin{thebibliography}{00}
\bibitem{b1} FastMCP Documentation, GitHub. Available: https://github.com/jlowin/fastmcp
\bibitem{b2} JaCoCo Documentation. Available: https://www.jacoco.org/jacoco/trunk/doc/
\bibitem{b3} Model Context Protocol Specification. Available: https://modelcontextprotocol.io
\end{thebibliography}

\end{document}

